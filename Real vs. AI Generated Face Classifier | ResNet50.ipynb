{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9251202,"sourceType":"datasetVersion","datasetId":5596873}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aruneembhowmick/real-vs-ai-generated-face-classifier-resnet50?scriptVersionId=200925808\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, we will build a binary classification model to distinguish between **real human faces** and **AI-generated images**. With the growing sophistication of AI-generated media, being able to accurately classify real and generated content has become increasingly important in fields like security, digital art, and media verification. Using a pre-trained **ResNet50** architecture, we'll fine-tune a model to tackle this problem with a custom dataset.\n\n## Key Steps\n1. **Data Preparation**: We'll start by organizing the dataset into training, validation, and test sets. Our dataset consists of two categories: \"Real Images\" and \"AI-Generated Images.\" We'll shuffle and split the data into 70% training, 15% validation, and 15% test sets to ensure robust evaluation of the model's performance.\n2. Model Development with Transfer Learning: Using **transfer learning** with **ResNet50**, we'll leverage the power of this pre-trained model, originally trained on **ImageNet**, to extract features from our images. By freezing the convolutional base and adding a custom fully connected layer, we fine-tune the model for our binary classification task.\n3. **Data Augmentation and Preprocessing**: We'll preprocess the images using the `ImageDataGenerator` to rescale pixel values and generate batches of data for training and validation. This step helps ensure the model is trained efficiently with normalized inputs.\n4. **Model Training and Evaluation**: The model will be trained using binary cross-entropy loss, with accuracy as the main evaluation metric. We'll track the training and validation accuracy to monitor the model's learning process over several epochs. Once training is complete, we'll evaluate the model on the test set to determine its generalization ability.\n5. **Visualizing Predictions**: Finally, we will implement a function to visually inspect the model's predictions by displaying real and AI-generated images alongside their true labels and predicted outcomes. This will help us analyze the model's performance in greater detail.","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/v2/resize:fit:1400/0*nwS9AyXvtcU1Ek1a.jpg)","metadata":{}},{"cell_type":"markdown","source":"# 1.) Dataset Preparation and Splitting\n\nWe organize our dataset of human faces by splitting it into training, validation, and test sets. Our dataset contains two categories of images: **Real Images** and **AI-Generated Images**. The code prepares these images for model training by distributing them into appropriate directories while maintaining a 70/15/15 split across training, validation, and test sets.\n\n## Key Steps\n1. **Defining Paths**: We begin by specifying the paths to the original dataset, which contains subdirectories for **Real Images** and **AI-Generated Images**. We then set up the paths for our output directories (`/kaggle/working/`) where the images will be copied for training, validation, and testing purposes.\n2. **Creating Directories**: The code creates subdirectories under `train`, `val`, and `test` for both **Real Images** and **AI-Generated Images** using the `os.makedirs()` function. The `exist_ok = True` ensures that directories are created only if they don't already exist.\n3. **Splitting Ratios**: We define the split ratios:\n    * **70%** for the training set\n    * **15%** for the validation set\n    * **15%** for the test set\n   \n   These ratios are stored as `train_split`, `val_split`, and `test_split` respectively.\n4. **Image Shuffling and Distribution**: The `split_and_copy_images` function handles the process of copying images from the source directories to the respective train, validation, and test folders. First, it retrieves a list of images in the source directory and shuffles them randomly using `random.shuffle()`, ensuring that the image split is random. Then, it calculates how many images will go into each split (train, validation, test) based on the total number of images and the defined split ratios. Finally, it copies each image into its designated directory using `shutil.copy()`. The function works for both **Real Images** and **AI-Generated Images**.","metadata":{}},{"cell_type":"code","source":"import shutil\nimport random\nimport os\n\ndataset_path = '/kaggle/input/human-faces-dataset/Human Faces Dataset'\nreal_images_path = os.path.join(dataset_path, 'Real Images')\nai_generated_images_path = os.path.join(dataset_path, 'AI-Generated Images')\n\noutput_path = '/kaggle/working/'\ntrain_path = os.path.join(output_path, 'train')\nval_path = os.path.join(output_path, 'val')\ntest_path = os.path.join(output_path, 'test')\n\nfor path in [train_path, val_path, test_path]:\n    os.makedirs(os.path.join(path, 'Real Images'), exist_ok = True)\n    os.makedirs(os.path.join(path, 'AI-Generated Images'), exist_ok = True)\n\ntrain_split = 0.7\nval_split = 0.15\ntest_split = 0.15\n\ndef split_and_copy_images(source_dir, dest_dirs, split_ratios):\n    images = os.listdir(source_dir)\n    random.shuffle(images)\n    \n    train_size = int(len(images) * split_ratios[0])\n    val_size = int(len(images) * split_ratios[1])\n    \n    for i, img in enumerate(images):\n        if i < train_size:\n            dest_dir = dest_dirs[0]\n        elif i < train_size + val_size:\n            dest_dir = dest_dirs[1]\n        else:\n            dest_dir = dest_dirs[2]\n            \n        shutil.copy(os.path.join(source_dir, img), os.path.join(dest_dir, img))\n\nsplit_and_copy_images(real_images_path, [os.path.join(train_path, 'Real Images'), os.path.join(val_path, 'Real Images'), os.path.join(test_path, 'Real Images')], [train_split, val_split, test_split])\nsplit_and_copy_images(ai_generated_images_path, [os.path.join(train_path, 'AI-Generated Images'), os.path.join(val_path, 'AI-Generated Images'), os.path.join(test_path, 'AI-Generated Images')], [train_split, val_split, test_split])","metadata":{"execution":{"iopub.status.busy":"2024-10-13T23:44:04.148011Z","iopub.execute_input":"2024-10-13T23:44:04.148731Z","iopub.status.idle":"2024-10-13T23:44:38.005528Z","shell.execute_reply.started":"2024-10-13T23:44:04.14869Z","shell.execute_reply":"2024-10-13T23:44:38.00464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result\nAfter running this code, we have our dataset neatly organized in the following folder structure:\n\n`/kaggle/working/\n    ├── train/\n    │   ├── Real Images/\n    │   └── AI-Generated Images/\n    ├── val/\n    │   ├── Real Images/\n    │   └── AI-Generated Images/\n    └── test/\n        ├── Real Images/\n        └── AI-Generated Images/`","metadata":{}},{"cell_type":"markdown","source":"# 2.) Data Preprocessing and Augmentation with `ImageDataGenerator`\nWe use TensorFlow’s `ImageDataGenerator` to handle the loading and preprocessing of images for our deep learning model. The generator simplifies the process of loading batches of images from directories, applying real-time data augmentation, and preparing them for training, validation, and testing. Below is an explanation of the key components:\n\n## Key Steps\n1. **Image Dimensions and Batch Size**: We specify the target dimensions for our images as **224 x 224** pixels (which is the default input size for the **ResNet50** model), and set the **batch size** to 32, meaning the model will process 32 images at a time during training.\n2. **Rescaling**: Each image’s pixel values are normalized by rescaling them from the range [0, 255] to the range [0, 1] using `rescale = 1./255`. This is important as it helps the neural network converge faster during training by standardizing the input values.\n3. **Data Generators**: We create three `ImageDataGenerator` instances for training, validation, and testing, each responsible for loading images from specific directories.\n    * `train_datagen`: Used for generating the training data.\n    * `val_datagen`: Used for generating validation data during model training.\n    * `test_datagen`: Used for generating test data to evaluate the model after training.\n4. **Flow from Directory**: The `flow_from_directory` function allows us to load images directly from folders:\n    * `train_path`, `val_path`, and `test_path`: These are the paths where the training, validation, and test images are stored, respectively.\n    * `target_size`: Specifies the dimensions to which all images will be resized (224 x 224 in this case).\n    * `batch_size`: Defines how many images will be processed in one iteration.\n    * `class_mode`: We use binary because this is a binary classification problem (real vs AI-generated images).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_height, img_width = 224, 224\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(rescale = 1./255)\nval_datagen = ImageDataGenerator(rescale = 1./255)\ntest_datagen = ImageDataGenerator(rescale = 1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size = (img_height, img_width),\n    batch_size = batch_size,\n    class_mode = 'binary'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_path,\n    target_size = (img_height, img_width),\n    batch_size = batch_size,\n    class_mode = 'binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_path,\n    target_size = (img_height, img_width),\n    batch_size = batch_size,\n    class_mode = 'binary'\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T23:44:38.007135Z","iopub.execute_input":"2024-10-13T23:44:38.007439Z","iopub.status.idle":"2024-10-13T23:44:51.893631Z","shell.execute_reply.started":"2024-10-13T23:44:38.007406Z","shell.execute_reply":"2024-10-13T23:44:51.892832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result \n* **Training Generator** (`train_generator`): Loads images from the training directory, resizes them to 224x224, normalizes their pixel values, and feeds them to the model in batches of 32.\n* **Validation Generator** (`val_generator`): Works similarly to the training generator but for validation data, used to monitor model performance during training.\n* **Test Generator** (`test_generator`): Loads the test data in the same manner and will be used for final evaluation after training.\n\n## Benefits of Using `ImageDataGenerator`\n* Efficient loading and processing of images in batches.\n* Real-time augmentation (if needed) during training.\n* Automatic shuffling of training data for better generalization.\n* Scalable for large datasets without loading everything into memory at once.\n\nThis process ensures that our images are properly preprocessed and ready for model training and evaluation.","metadata":{}},{"cell_type":"markdown","source":"# 3.) Model Architecture: Transfer Learning with ResNet50\n\nWe leverage **transfer learning** to build a deep learning model for binary classification. Specifically, we use the **ResNet50** architecture as the base model, which is pretrained on the **ImageNet** dataset. Here's a breakdown of the steps and layers involved:\n\n## Key Steps\n1. **Loading the Pretrained ResNet50 Model**: We begin by loading the **ResNet50** model using `tensorflow.keras.applications.ResNet50`. The key parameters include:\n    * `weights = 'imagenet'`: This means that we are using the weights that were pre-trained on the ImageNet dataset.\n    * `include_top = False`: We exclude the top classification layer because we will be adding our custom classification layers.\n    * `input_shape = (img_height, img_width, 3)`: We define the input shape for the images, where `img_height` and `img_width` are the height and width of our input images, and `3` corresponds to the RGB channels.\n2. **Adding Custom Layers**: After obtaining the output of the base ResNet50 model, we add our own layers:\n    * `GlobalAveragePooling2D()`: This layer performs global average pooling, reducing the spatial dimensions of the output from the ResNet50 model while retaining important features.\n    * `Dense(128, activation = 'relu')`: We add a fully connected dense layer with 128 units and ReLU activation to introduce non-linearity and learn higher-level patterns.\n    * `Dense(1, activation = 'sigmoid')`: Finally, we add a dense layer with a single output neuron and a sigmoid activation function for binary classification.\n3. **Freezing the Pretrained Layers**: Since ResNet50 was pre-trained on a large dataset (ImageNet), we **freeze its layers** to prevent them from being updated during training. This allows us to focus training on the newly added custom layers. Freezing is achieved by setting `layer.trainable = False` for each layer in the base model.\n4. **Compiling the Model**: The model is compiled with the following parameters:\n    * **Optimizer**: We use the Adam optimizer, which is well-suited for deep learning tasks and adapts the learning rate during training.\n    * **Loss Function**: `binary_crossentropy` is used as the loss function because this is a binary classification problem.\n    * **Metrics**: We monitor accuracy during training to evaluate the model's performance.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\nbase_model = ResNet50(weights = 'imagenet', include_top = False, input_shape = (img_height, img_width, 3))\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation ='relu')(x)\npredictions = Dense(1, activation ='sigmoid')(x)\n\nmodel = Model(inputs = base_model.input, outputs = predictions)\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-10-13T23:44:51.894947Z","iopub.execute_input":"2024-10-13T23:44:51.896086Z","iopub.status.idle":"2024-10-13T23:44:54.707419Z","shell.execute_reply.started":"2024-10-13T23:44:51.896036Z","shell.execute_reply":"2024-10-13T23:44:54.706486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result (Generated Model Architecture):\n* Pretrained ResNet50 (with ImageNet weights)\n* Global Average Pooling layer\n* Dense layer with 128 units (ReLU activation)\n* Dense output layer with 1 unit (Sigmoid activation) for binary classification\n\nThis setup allows us to benefit from ResNet50's ability to extract powerful features from images, while fine-tuning a small custom network on top of it to perform our specific binary classification task.","metadata":{}},{"cell_type":"markdown","source":"# 4.) Model Training with `fit()`\n\nWe train our binary classification model using the `.fit()` method from **TensorFlow**. The training process involves feeding batches of images from the training set into the model and using the validation set to evaluate the model's performance after each epoch.\n\n## Key Components\n1. **Training Data** (`train_generator`): The `train_generator` supplies the batches of preprocessed images (from the training directory) to the model during training. These batches are loaded and processed by the `ImageDataGenerator`, ensuring that images are fed into the model in a normalized format and shuffled for better generalization.\n2. **Validation Data** (`val_generator`): The `validation_data` parameter is set to the `val_generator`, which provides validation images after each epoch. This helps in monitoring how well the model is performing on unseen data, allowing us to track overfitting or underfitting during training.\n3. **Epochs** (`epochs = 10`): The epochs parameter defines how many times the model will see the entire dataset. In this case, the model will go through 10 full training cycles. More epochs generally help the model learn better, but too many epochs can lead to overfitting, where the model becomes too specific to the training data.\n\n## **Training Process**\n* **Forward Pass**: For each batch of images in `train_generator`, the model makes predictions and calculates the loss.\n* **Backward Pass (Backpropagation)**: Based on the calculated loss, the model adjusts its weights using the optimizer (in this case, Adam) to reduce the error in future predictions.\n* **Validation Check**: At the end of each epoch, the model evaluates itself on the validation set using `val_generator`. This helps us monitor how well the model generalizes to unseen data.\n\nThe `.fit()` method stores the training history in the history object, which includes the following:\n* Training loss and accuracy for each epoch.\n* Validation loss and accuracy, which give us insights into how the model performs on unseen data.\n\nThis setup allows us to visualize the model's learning process, such as observing whether the model is improving over time, converging to a solution, or overfitting.","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    train_generator,\n    validation_data = val_generator,\n    epochs = 10\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T23:44:54.717897Z","iopub.execute_input":"2024-10-13T23:44:54.71822Z","iopub.status.idle":"2024-10-13T23:48:53.077794Z","shell.execute_reply.started":"2024-10-13T23:44:54.718188Z","shell.execute_reply":"2024-10-13T23:48:53.077025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.) Model Evaluation with `evaluate()`\nWe evaluate the model's performance on the test set to see how well it generalizes to completely unseen data. This is done using the `model.evaluate()` function, which computes the loss and accuracy on the test data.\n\n## Key Components\n1. **Test Data (`test_generator`)**: The `test_generator` provides batches of test images from the test directory, which the model has not seen during training or validation. These images are normalized (rescaled) and resized similarly to the training and validation sets.\n2. **Evaluation Metrics**: The `evaluate()` method returns two key metrics:\n* `test_loss`: This represents how well the model performs on the test data in terms of loss (binary cross-entropy in this case). A lower loss indicates better performance.\n* `test_acc`: This is the accuracy of the model on the test set, showing how many predictions were correct compared to the actual labels. A higher accuracy score indicates that the model generalizes well to unseen data.\n3. **Printing the Results**: After evaluating the model, we print the test loss and test accuracy to assess the model's overall performance.\n\n## Key Points\n* **Test Loss**: This value measures how well the model fits the test data. If the test loss is significantly higher than the training or validation loss, it may indicate overfitting.\n* **Test Accuracy**: This provides a clear metric to understand the model's predictive power. A high test accuracy (close to 100%) indicates that the model can generalize well, while a low test accuracy might suggest the need for further tuning or that the dataset is challenging.\n\nEvaluating the model on the test set gives a final confirmation of how well the model performs in real-world scenarios, making it an essential step in the model development process.","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_generator)\nprint(f'Test Loss: {test_loss}')\nprint(f'Test Accuracy: {test_acc}')","metadata":{"execution":{"iopub.status.busy":"2024-10-13T23:48:53.079151Z","iopub.execute_input":"2024-10-13T23:48:53.079471Z","iopub.status.idle":"2024-10-13T23:48:59.372797Z","shell.execute_reply.started":"2024-10-13T23:48:53.079437Z","shell.execute_reply":"2024-10-13T23:48:59.371894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.) Displaying Model Predictions with Correct Labels\n\nWe develop a function intended to provide a clear visualization of the model's predictions for a batch of images, allowing us to see how well it performs and where it may be making errors. Here's a detailed breakdown of the steps and logic involved:\n\n## Key Components \n1. **Loading a Batch of Images**: The function begins by calling `next(generator)` to load a batch of images and their corresponding true labels from the specified generator (in this case, the validation generator).\n    * `images`: A batch of images (converted into a NumPy array) that will be used for prediction.\n    * `true_labels`: The true labels for these images, indicating whether they are \"Real\" or \"AI-Generated.\"\n    \n2. **Model Predictions**: Using the model, we generate predictions for each image in the batch. These predictions are probabilities, and by applying a threshold (0.5 by default), we classify them into either the \"Real\" or \"AI-Generated\" categories.\n    * `predicted_labels`: Binary labels created by thresholding the predicted probabilities (i.e., if the predicted probability is ≥ 0.5, the image is classified as \"Real\"; otherwise, it is classified as \"AI-Generated\").\n    \n3. **Handling Labels**: The labels are processed for display:\n    * **True labels and predicted labels** are converted into human-readable strings (\"Real\" or \"AI Generated\") to make the output more intuitive.\n    * **Correct predictions** are identified by comparing the true labels with the predicted labels.\n    \n4. **Creating a Table**: The data for each image (index, true label, predicted label, and whether the prediction was correct) is stored in a pandas `DataFrame`. This table is returned at the end of the function, making it easy to review and analyze the model's performance.\n\n5. **Plotting Images with Predictions**: The function generates a plot that displays the first 20 images from the batch, each with:\n    * The true label.\n    * The predicted label.\n    * Whether the prediction was correct (True/False).\n    \n    The images are displayed in a grid layout using matplotlib to give a visual summary of the model's performance.\n\n\n## Benefits\n* **Visual Feedback**: Seeing both the images and the predictions allows you to better understand how the model is performing. You can easily spot where it makes mistakes and identify patterns in its errors.\n* **Comprehensive Table**: The accompanying table gives a quick overview of which predictions were correct and incorrect, helping in error analysis.\n* **Customizable**: The function allows you to specify how many images to display (via the num_images parameter) and adjust the threshold for classification.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef show_predictions_table_with_correct_labels(model, generator, num_images = 20, threshold = 0.5):\n    images, true_labels = next(generator)\n    images, true_labels = images[:num_images], true_labels[:num_images]\n    \n    predictions = model.predict(images)\n    predicted_labels = (predictions >= threshold).astype(int)\n  \n    if true_labels.ndim > 1:\n        true_labels = np.argmax(true_labels, axis=1)\n    \n    true_label_names = [\"Real\" if label == 1 else \"AI Generated\" for label in true_labels]\n    predicted_label_names = [\"Real\" if label == 1 else \"AI Generated\" for label in predicted_labels]\n    correct_predictions = predicted_labels.flatten() == true_labels.flatten()\n\n    df = pd.DataFrame({\n        \"Image Index\": list(range(num_images)),\n        \"True Label\": true_label_names,\n        \"Predicted Label\": predicted_label_names,\n        \"Correct\": correct_predictions\n    })\n\n    fig, axes = plt.subplots(5, 4, figsize = (15, 12))\n\n    for i, ax in enumerate(axes.flat):\n        ax.imshow(images[i])\n        ax.set_title(f\"True: {df['True Label'][i]}\\nPred: {df['Predicted Label'][i]}\\nCorrect: {df['Correct'][i]}\")\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    return df\n\nshow_predictions_table_with_correct_labels(model, val_generator, num_images = 20, threshold = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T23:48:59.373875Z","iopub.execute_input":"2024-10-13T23:48:59.374199Z","iopub.status.idle":"2024-10-13T23:49:06.565058Z","shell.execute_reply.started":"2024-10-13T23:48:59.374166Z","shell.execute_reply":"2024-10-13T23:49:06.56415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, we successfully developed a deep learning model capable of distinguishing between **real human faces** and **AI-generated images** using a transfer learning approach with **ResNet50**. By leveraging the pre-trained model, we were able to efficiently extract features from images and fine-tune the model for our binary classification task.\n\n### Key Takeaways:\n\n1. **Data Preparation and Splitting**:  \n   We effectively organized and split the dataset into training, validation, and test sets, ensuring a balanced approach for model training and performance evaluation.\n\n2. **Transfer Learning with ResNet50**:  \n   Utilizing the ResNet50 architecture enabled us to capitalize on its feature extraction capabilities, allowing us to focus on training the classifier without having to build the entire model from scratch. By freezing the convolutional layers and training custom dense layers, we tailored the model to the specific task of face classification.\n\n3. **Model Training and Performance**:  \n   Our model achieved good results during training, with accuracy metrics indicating that it learned to differentiate between real and AI-generated images effectively. We validated the model's performance not only through quantitative metrics but also by visually inspecting the predictions for a subset of images.\n\n4. **Visualization of Predictions**:  \n   The visualization of model predictions, along with correct labels, allowed us to gain insights into the areas where the model excelled and where it struggled. This visual approach complements the accuracy metrics and helps in understanding the strengths and limitations of the model.\n\n### Future Work:\n\n- **Data Augmentation**: To further improve the model's robustness, we could explore more advanced data augmentation techniques, such as rotation, flipping, and contrast adjustment, to simulate various real-world conditions.\n  \n- **Fine-tuning the Entire Model**: In this notebook, we froze the ResNet50 base model layers. Unfreezing some or all of the layers and fine-tuning the entire network could lead to even better performance.\n  \n- **Exploring Other Architectures**: While ResNet50 performed well, experimenting with other architectures such as **EfficientNet** or **Inception** could provide insights into whether these models perform better in this specific task.","metadata":{}}]}